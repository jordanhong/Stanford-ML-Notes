%        File: Stanford_Machine_Learning_Notes.tex
%     Created: Mon May 18 11:00 pm 2020 E
% Last Change: Mon May 18 11:00 pm 2020 E
%
\documentclass[letter]{article}
\title{%
    Machine Learning\\
    \large Stanford University \\
    Professor Andrew Ng}

\author{Jordan Hong}
\date{\today}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}


\begin{document}
\maketitle
\tableofcontents

\section{Introduction}
    \subsection{What is Machine Learning}
    
    \begin{enumerate}
        \item Machine Learning 
            \begin{itemize}
                \item Grew out of work in Artificial Intelligence (AI)
                \item New capabilities for computers
            \end{itemize}
        \item Examples: 
            \begin{itemize}
                \item database mining
                \item applications can't programby hand (handwriting recognition, Natural Language Processing (NLP), Computer Vision) 
                \item Neuromorphic applications
            \end{itemize}
           
        \item Definition
            \begin{itemize}
                \item Arthur Samuel(1959) \\
                    \begin{quote}
                        Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed.

                    \end{quote}
                \item Tom Mitchell(1998) \\
                    \begin{quote}
                         Well-posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. 

                    \end{quote}
           \end{itemize}
        \item Machine Learning in this course:
            \begin{enumerate}
                \item Suupervised Learning
                \item Unsupervised Learning
                \item Others: reinforcement learning, recommender systems
                \item Practical application techniques
            \end{enumerate}
    \end{enumerate}

    
    \subsection{Supervised Learning}

        In supervised learning, the \emph{the right answer} is given. For example:
        \begin{enumerate}
            \item Regression: predict real-valued output.
            \item Classification: predict discrete-valued output.
        \end{enumerate}

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{image/supervised-learning.png}
        \caption{Supervised Learning}
        \label{fig:supervised-learning}
    \end{figure}

    \subsection{Unsupervised Learning}
    The right answer is not given, e.g. cocktail problem (distinguishing two voices from an audio file.)

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{image/unsupervised-learning.png}
        \caption{Unsupervised learning}
        \label{fig:unsupervised-learning}
    \end{figure}
\section{Linear Regression with One Variable}
    \subsection{Model Representation}
        \subsubsection{Notations}
            For a training set:
           \begin{itemize}
               \item \textbf{m} = Number of training examples.
               \item \textbf{x} = ``input'' variable / features.
               \item \textbf{y} = ``output'' variables / ``target'' variable.
               \item \textbf{(x,y)} - one training example.
               \item \textbf{(x\textsuperscript{i},y\textsuperscript{i})} denotes the i\textsuperscript{th} training example 

           \end{itemize}

        \subsubsection{Hypothesis Function}
        
           A hypothesis function (h) maps input (x) to estimated output (y).
           How do we represent h?

           \begin{equation} 
               \boxed{ 
                   \textbf{Hypothesis Function}\hspace{10pt}  h_\theta (x) = \theta_0 + \theta_1x
           }
              \label{eq:hypothesis}
           \end{equation}

           We can apply \emph{Univariate linear regression} with respect to x. 
    \subsection{Cost Function}

    Recall \ref{eq:hypothesis}. The $\theta_i$s are parameters we have to choose. The intuition is is that we want to choose $\theta_i$ s such that h\textsubscript{$\theta$} is closest to y for our training examples (x,y).
    

      \begin{equation} 
          \boxed{ 
              \textbf{Cost Function}\hspace{10pt} J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)} )^2
      }
          \label{eq:cost}
      \end{equation}
      

      \par \textbf{Summary} 
      \begin{enumerate}
          \item \textbf{Hypothesis  }$h_\theta (x) = \theta_0 + \theta_1x$
          \item \textbf{Parameters } $\theta_0, \theta_1$
          \item \textbf{Cost Function }$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)} )^2$
          \item \textbf{Goal } $\min_{\theta_0, \theta_1} J(\theta+0, \theta_1)$
    
      \end{enumerate}
        

          
   
    
    \subsection{Gradient Descent}
        \subsubsection{Intuition}
            \begin{enumerate}
                \item We have some function $J(\theta_0, \theta_1)$, we want to $\min_{\theta_0, \theta_1} J (\theta_0, \theta_1)$
                \item Outline: start with some $\theta_0, \theta_1$, keep changing  $\theta_0, \theta_1$ to reduce $J(\theta_0, \theta_1)$ until we end up at a minimum. 
            \end{enumerate}
        \subsubsection{Gradient Descent Algorithm}
            \textbf{Algorithm} \\
                repeat until convergence\{  
                    \[ \theta_j := \theta_j - \alpha \frac{\partial }{\partial \theta_j} J(\theta_0, \theta_1)\mbox{\hspace{10pt} (for j=0 and j=1)} 
                   .\] \}
        \\


           \textbf{Notes}
               \begin{enumerate}
                   \item the := denotes non-blocking assignment, i.e. simultaneously updates $\theta_0 and \theta_1$ 
                   \item We use the derivative to find a local minimum. 
                   \item $\alpha$ denotes the learning rate. Gradient descent can converge to a local minimum even when the learning rate $\alpha$ is fixed. As we approach a local minimum, gradient descent will automatically take smaller steps. Therefore it is not needed to decrease $\alpha$ over time. 
               \end{enumerate}

       \subsubsection{Gradient Descent with Linear Regression}
     
       Recall, we have:
       \begin{enumerate}
           \item Gradient Descent Algorithm: \\ 
               \linebreak
               repeat until convergence\{  
                    \[ \theta_j := \theta_j - \alpha \frac{\partial }{\partial \theta_j} J(\theta_0, \theta_1)\mbox{\hspace{10pt} (for j=0 and j=1)} 
                   .\] \}


           \item Linear Regression Model:
               \begin{center}
               \[
               h_\theta (x) = \theta_0 + \theta_1x

               J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)} )^2

               .\]             

               \end{center}

       \end{enumerate}
            
                 
                
    We can substitute the above equations, which gives us:
       \begin{center}
            \[
            \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)} )

            \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)} ) \cdot x^{(i)}

            .\]  

       
       \end{center} 
    
\end{document}


